<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>pbcore.io.dataset &mdash; pbcoretools 0.1.0 documentation</title>
    
    <link rel="stylesheet" href="_static/pacbio.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '0.1.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="pbcoretools 0.1.0 documentation" href="index.html" /> 
  </head>
  <body role="document">
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="nav-item nav-item-0"><a href="index.html">pbcoretools 0.1.0 documentation</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="pbcore-io-dataset">
<h1>pbcore.io.dataset<a class="headerlink" href="#pbcore-io-dataset" title="Permalink to this headline">¶</a></h1>
<p>The Python DataSet XML API is designed to be a lightweight interface for
creating, opening, manipulating and writing DataSet XML files. It provides both
a native Python API and console entry points for use in manual dataset curation
or as a resource for P_Module developers.</p>
<p>The API and console entry points are designed with the set operations one might
perform on the various types of data held by a DataSet XML in mind: merge,
split, write etc. While various types of DataSets can be found in XML files,
the API (and in a way the console entry point, dataset.py) has DataSet as its
base type, with various subtypes extending or replacing functionality as
needed.</p>
</div>
<div class="section" id="console-entry-point-usage">
<h1>Console Entry Point Usage<a class="headerlink" href="#console-entry-point-usage" title="Permalink to this headline">¶</a></h1>
<p>The following entry points are available through the main script: dataset.py:</p>
<div class="highlight-python"><div class="highlight"><pre>usage: dataset.py [-h] [-v] [--debug]
                  {create,filter,merge,split,validate,loadstats,consolidate}
                  ...

Run dataset.py by specifying a command.

optional arguments:
  -h, --help            show this help message and exit
  -v, --version         show program&#39;s version number and exit
  --debug               Turn on debug level logging

DataSet sub-commands:
  {create,filter,merge,split,validate,loadstats,consolidate}
                        Type {command} -h for a command&#39;s options
</pre></div>
</div>
<p>Create:</p>
<div class="highlight-python"><div class="highlight"><pre>usage: dataset.py create [-h] [--type DSTYPE] [--novalidate] [--relative]
                         outfile infile [infile ...]

Create an XML file from a fofn or bam

positional arguments:
  outfile        The XML to create
  infile         The fofn or BAM file(s) to make into an XML

optional arguments:
  -h, --help     show this help message and exit
  --type DSTYPE  The type of XML to create
  --novalidate   Don&#39;t validate the resulting XML, don&#39;t touch paths
  --relative     Make the included paths relative instead of absolute (not
                 compatible with --novalidate)
</pre></div>
</div>
<p>Filter:</p>
<div class="highlight-python"><div class="highlight"><pre>usage: dataset.py filter [-h] infile outfile filters [filters ...]

Add filters to an XML file. Suggested fields: [&#39;bcf&#39;, &#39;bcq&#39;, &#39;bcr&#39;,
&#39;length&#39;, &#39;pos&#39;, &#39;qend&#39;, &#39;qname&#39;, &#39;qstart&#39;, &#39;readstart&#39;, &#39;rname&#39;, &#39;rq&#39;,
&#39;tend&#39;, &#39;tstart&#39;, &#39;zm&#39;]. More expensive fields: [&#39;accuracy&#39;, &#39;bc&#39;, &#39;movie&#39;,
&#39;qs&#39;]

positional arguments:
  infile      The xml file to filter
  outfile     The resulting xml file
  filters     The values and thresholds to filter (e.g. &#39;rq&gt;0.85&#39;)

optional arguments:
  -h, --help  show this help message and exit
</pre></div>
</div>
<p>Union:</p>
<div class="highlight-python"><div class="highlight"><pre>usage: dataset.py union [-h] outfile infiles [infiles ...]

Combine XML (and BAM) files

positional arguments:
  outfile     The resulting XML file
  infiles     The XML files to merge

optional arguments:
  -h, --help  show this help message and exit
</pre></div>
</div>
<p>Validate:</p>
<div class="highlight-python"><div class="highlight"><pre>usage: dataset.py validate [-h] infile

Validate ResourceId files (XML validation only available in testing)

positional arguments:
  infile      The XML file to validate

optional arguments:
  -h, --help  show this help message and exit
</pre></div>
</div>
<p>Load PipeStats:</p>
<div class="highlight-python"><div class="highlight"><pre>usage: dataset.py loadstats [-h] [--outfile OUTFILE] infile statsfile

Load an sts.xml file into a DataSet XML file

positional arguments:
  infile             The XML file to modify
  statsfile          The .sts.xml file to load

optional arguments:
  -h, --help         show this help message and exit
  --outfile OUTFILE  The XML file to output
</pre></div>
</div>
<p>Split:</p>
<div class="highlight-python"><div class="highlight"><pre>usage: dataset.py split [-h] [--contigs] [--chunks CHUNKS] [--subdatasets]
                        [--outdir OUTDIR]
                        infile ...

Split the dataset

positional arguments:
  infile           The xml file to split
  outfiles         The resulting xml files

optional arguments:
  -h, --help       show this help message and exit
  --contigs        Split on contigs
  --chunks CHUNKS  Split contigs into &lt;chunks&gt; total windows
  --subdatasets    Split on subdatasets
  --outdir OUTDIR  Specify an output directory
</pre></div>
</div>
<p>Consolidate:</p>
<div class="highlight-python"><div class="highlight"><pre>usage: dataset.py consolidate [-h] [--numFiles NUMFILES] [--noTmp]
                              infile datafile xmlfile

Consolidate the XML files

positional arguments:
  infile               The XML file to consolidate
  datafile             The resulting data file
  xmlfile              The resulting XML file

optional arguments:
  -h, --help           show this help message and exit
  --numFiles NUMFILES  The number of data files to produce (1)
  --noTmp              Don&#39;t copy to a tmp location to ensure local disk
                       use
</pre></div>
</div>
</div>
<div class="section" id="usage-examples">
<h1>Usage Examples<a class="headerlink" href="#usage-examples" title="Permalink to this headline">¶</a></h1>
<div class="section" id="filter-reads-cli-version">
<h2>Filter Reads (CLI version)<a class="headerlink" href="#filter-reads-cli-version" title="Permalink to this headline">¶</a></h2>
<p>In this scenario we have one or more bam files worth of subreads, aligned or
otherwise, that we want to filter and put in a single bam file. This is
possible using the CLI with the following steps, starting with a DataSet XML
file:</p>
<div class="highlight-python"><div class="highlight"><pre># usage: dataset.py filter &lt;in_fn.xml&gt; &lt;out_fn.xml&gt; &lt;filters&gt;
dataset.py filter in_fn.subreadset.xml filtered_fn.subreadset.xml &#39;rq&gt;0.85&#39;

# usage: dataset.py consolidate &lt;in_fn.xml&gt; &lt;out_data_fn.bam&gt; &lt;out_fn.xml&gt;
dataset.py consolidate filtered_fn.subreadset.xml consolidate.subreads.bam out_fn.subreadset.xml
</pre></div>
</div>
<p>The filtered DataSet and the consolidated DataSet should be read for read
equivalent when used with SMRT Analysis software.</p>
</div>
<div class="section" id="filter-reads-api-version">
<h2>Filter Reads (API version)<a class="headerlink" href="#filter-reads-api-version" title="Permalink to this headline">¶</a></h2>
<p>The API version of filtering allows for more advanced filtering criteria:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">ss</span> <span class="o">=</span> <span class="n">SubreadSet</span><span class="p">(</span><span class="s">&#39;in_fn.subreadset.xml&#39;</span><span class="p">)</span>
<span class="n">ss</span><span class="o">.</span><span class="n">filters</span><span class="o">.</span><span class="n">addRequirement</span><span class="p">(</span><span class="n">rname</span><span class="o">=</span><span class="p">[(</span><span class="s">&#39;=&#39;</span><span class="p">,</span> <span class="s">&#39;E.faecalis.2&#39;</span><span class="p">),</span>
                                 <span class="p">(</span><span class="s">&#39;=&#39;</span><span class="p">,</span> <span class="s">&#39;E.faecalis.2&#39;</span><span class="p">)],</span>
                          <span class="n">tStart</span><span class="o">=</span><span class="p">[(</span><span class="s">&#39;&lt;&#39;</span><span class="p">,</span> <span class="s">&#39;99&#39;</span><span class="p">),</span>
                                  <span class="p">(</span><span class="s">&#39;&lt;&#39;</span><span class="p">,</span> <span class="s">&#39;299&#39;</span><span class="p">)],</span>
                          <span class="n">tEnd</span><span class="o">=</span><span class="p">[(</span><span class="s">&#39;&gt;&#39;</span><span class="p">,</span> <span class="s">&#39;0&#39;</span><span class="p">),</span>
                                <span class="p">(</span><span class="s">&#39;&gt;&#39;</span><span class="p">,</span> <span class="s">&#39;200&#39;</span><span class="p">)])</span>
</pre></div>
</div>
<p>Produces the following conditions for a read to be considered passing:</p>
<p>(rname = E.faecalis.2 AND tstart &lt; 99 AND tend &gt; 0)
OR
(rname = E.faecalis.2 AND tstart &lt; 299 AND tend &gt; 200)</p>
<p>You can add sets of filters by providing equal length lists of requirements for
each filter.</p>
<p>Additional requirements added singly will be added to all filters:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">ss</span><span class="o">.</span><span class="n">filters</span><span class="o">.</span><span class="n">addRequirement</span><span class="p">(</span><span class="n">rq</span><span class="o">=</span><span class="p">[(</span><span class="s">&#39;&gt;&#39;</span><span class="p">,</span> <span class="s">&#39;0.85&#39;</span><span class="p">)])</span>
</pre></div>
</div>
<p>(rname = E.faecalis.2 AND tstart &lt; 99 AND tend &gt; 0 AND rq &gt; 0.85)
OR
(rname = E.faecalis.2 AND tstart &lt; 299 AND tend &gt; 100 AND rq &gt; 0.85)</p>
<p>Additional requirements added with a plurality of options will duplicate the
previous requiremnts for each option:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">ss</span><span class="o">.</span><span class="n">filters</span><span class="o">.</span><span class="n">addRequirement</span><span class="p">(</span><span class="n">length</span><span class="o">=</span><span class="p">[(</span><span class="s">&#39;&gt;&#39;</span><span class="p">,</span> <span class="mi">500</span><span class="p">),</span> <span class="p">(</span><span class="s">&#39;&gt;&#39;</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)])</span>
</pre></div>
</div>
<p>(rname = E.faecalis.2 AND tstart &lt; 99 AND tend &gt; 0 AND rq &gt; 0.85 AND length &gt; 500)
OR
(rname = E.faecalis.2 AND tstart &lt; 299 AND tend &gt; 100 AND rq &gt; 0.85 AND length &gt; 500)
OR
(rname = E.faecalis.2 AND tstart &lt; 99 AND tend &gt; 0 AND rq &gt; 0.85 AND length &gt; 1000)
OR
(rname = E.faecalis.2 AND tstart &lt; 299 AND tend &gt; 100 AND rq &gt; 0.85 AND length &gt; 1000)</p>
<p>Of course you can always wipe the filters and start over:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">ss</span><span class="o">.</span><span class="n">filters</span> <span class="o">=</span> <span class="bp">None</span>
</pre></div>
</div>
<p>Consolidation is more similar to the CLI version:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">ss</span><span class="o">.</span><span class="n">consolidate</span><span class="p">(</span><span class="s">&#39;cons.bam&#39;</span><span class="p">)</span>
<span class="n">ss</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s">&#39;cons.xml&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="resequencing-pipeline-cli-version">
<h2>Resequencing Pipeline (CLI version)<a class="headerlink" href="#resequencing-pipeline-cli-version" title="Permalink to this headline">¶</a></h2>
<p>In this scenario, we have two movies worth of subreads in two SubreadSets that
we want to align to a reference, merge together, split into DataSet chunks by
contig, then send through quiver on a chunkwise basis (in parallel).</p>
<ol class="arabic">
<li><p class="first">Align each movie to the reference, producing a dataset with one bam file for
each execution:</p>
<div class="highlight-python"><div class="highlight"><pre>pbalign movie1.subreadset.xml referenceset.xml movie1.alignmentset.xml
pbalign movie2.subreadset.xml referenceset.xml movie2.alignmentset.xml
</pre></div>
</div>
</li>
<li><p class="first">Merge the files into a FOFN-like dataset (bams aren&#8217;t touched):</p>
<div class="highlight-python"><div class="highlight"><pre># dataset.py merge &lt;out_fn&gt; &lt;in_fn&gt; [&lt;in_fn&gt; &lt;in_fn&gt; ...]
dataset.py merge merged.alignmentset.xml movie1.alignmentset.xml movie2.alignmentset.xml
</pre></div>
</div>
</li>
<li><p class="first">Split the dataset into chunks by contig (rname) (bams aren&#8217;t touched). Note
that supplying output files splits the dataset into that many output files
(up to the number of contigs), with multiple contigs per file. Not supplying
output files splits the dataset into one output file per contig, named
automatically. Specifying a number of chunks instead will produce that many
files, with contig or even sub contig (reference window) splitting.:</p>
<div class="highlight-python"><div class="highlight"><pre>dataset.py split --contigs --chunks 8 merged.alignmentset.xml
</pre></div>
</div>
</li>
<li><p class="first">Quiver then consumes these chunks:</p>
<div class="highlight-python"><div class="highlight"><pre>variantCaller.py --alignmentSetRefWindows --referenceFileName referenceset.xml --outputFilename chunk1consensus.fasta --algorithm quiver chunk1contigs.alignmentset.xml
variantCaller.py --alignmentSetRefWindows --referenceFileName referenceset.xml --outputFilename chunk2consensus.fasta --algorithm quiver chunk2contigs.alignmentset.xml
</pre></div>
</div>
</li>
</ol>
<p>The chunking works by duplicating the original merged dataset (no bam
duplication) and adding filters to each duplicate such that only reads
belonging to the appropriate contigs are emitted. The contigs are distributed
amongst the output files in such a way that the total number of records per
chunk is about even.</p>
<div class="section" id="tangential-information">
<h3>Tangential Information<a class="headerlink" href="#tangential-information" title="Permalink to this headline">¶</a></h3>
<p>DataSet.refNames (which returns a list of reference names available in the
dataset) is also subject to the filtering imposed during the split. Therefore
you wont be running through superfluous (and apparently unsampled) contigs to
get the reads in this chunk. The DataSet.records generator is also subject to
filtering, but not as efficiently as readsInRange. If you do not have a
reference window, readsInReference() is also an option.</p>
<p>As the bam files are never touched, each dataset contains all the information
necessary to access all reads for all contigs. Doing so on these filtered
datasets would require disabling the filters first:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">dset</span><span class="o">.</span><span class="n">disableFilters</span><span class="p">()</span>
</pre></div>
</div>
<p>Or removing the specific filter giving you problems:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">dset</span><span class="o">.</span><span class="n">filters</span><span class="o">.</span><span class="n">removeRequirement</span><span class="p">(</span><span class="s">&#39;rname&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="resequencing-pipeline-api-version">
<h2>Resequencing Pipeline (API version)<a class="headerlink" href="#resequencing-pipeline-api-version" title="Permalink to this headline">¶</a></h2>
<p>In this scenario, we have two movies worth of subreads in two SubreadSets that
we want to align to a reference, merge together, split into DataSet chunks by
contig, then send through quiver on a chunkwise basis (in parallel). We want to
do them using the API, rather than the CLI.</p>
<ol class="arabic">
<li><p class="first">Align each movie to the reference, producing a dataset with one bam file for
each execution</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># CLI (or see pbalign API):</span>
<span class="n">pbalign</span> <span class="n">movie1</span><span class="o">.</span><span class="n">subreadset</span><span class="o">.</span><span class="n">xml</span> <span class="n">referenceset</span><span class="o">.</span><span class="n">xml</span> <span class="n">movie1</span><span class="o">.</span><span class="n">alignmentset</span><span class="o">.</span><span class="n">xml</span>
<span class="n">pbalign</span> <span class="n">movie2</span><span class="o">.</span><span class="n">subreadset</span><span class="o">.</span><span class="n">xml</span> <span class="n">referenceset</span><span class="o">.</span><span class="n">xml</span> <span class="n">movie2</span><span class="o">.</span><span class="n">alignmentset</span><span class="o">.</span><span class="n">xml</span>
</pre></div>
</div>
</li>
<li><p class="first">Merge the files into a FOFN-like dataset (bams aren&#8217;t touched)</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># API, filename_list is dummy data:</span>
<span class="n">filename_list</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;movie1.alignmentset.xml&#39;</span><span class="p">,</span> <span class="s">&#39;movie2.alignmentset.xml&#39;</span><span class="p">]</span>

<span class="c"># open:</span>
<span class="n">dsets</span> <span class="o">=</span> <span class="p">[</span><span class="n">AlignmentSet</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span> <span class="k">for</span> <span class="n">fn</span> <span class="ow">in</span> <span class="n">filename_list</span><span class="p">]</span>
<span class="c"># merge with + operator:</span>
<span class="n">dset</span> <span class="o">=</span> <span class="nb">reduce</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">,</span> <span class="n">dsets</span><span class="p">)</span>

<span class="c"># OR:</span>
<span class="n">dset</span> <span class="o">=</span> <span class="n">AlignmentSet</span><span class="p">(</span><span class="o">*</span><span class="n">filename_list</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p class="first">Split the dataset into chunks by contigs (or subcontig windows)</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># split:</span>
<span class="n">dsets</span> <span class="o">=</span> <span class="n">dset</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">contigs</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">chunks</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p class="first">Quiver then consumes these chunks</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># write out if you need to (or pass directly to quiver API):</span>
<span class="n">outfilename_list</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;chunk1contigs.alignmentset.xml&#39;</span><span class="p">,</span> <span class="s">&#39;chunk2contigs.alignmentset.xml&#39;</span><span class="p">]</span>
<span class="c"># write with &#39;write&#39; method:</span>
<span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="p">(</span><span class="n">ds</span><span class="p">,</span> <span class="n">nm</span><span class="p">):</span> <span class="n">ds</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">nm</span><span class="p">),</span> <span class="nb">zip</span><span class="p">(</span><span class="n">dsets</span><span class="p">,</span> <span class="n">outfilename_list</span><span class="p">))</span>

<span class="c"># CLI (or see quiver API):</span>
<span class="n">variantCaller</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">alignmentSetRefWindows</span> <span class="o">--</span><span class="n">referenceFileName</span> <span class="n">referenceset</span><span class="o">.</span><span class="n">xml</span> <span class="o">--</span><span class="n">outputFilename</span> <span class="n">chunk1consensus</span><span class="o">.</span><span class="n">fasta</span> <span class="o">--</span><span class="n">algorithm</span> <span class="n">quiver</span> <span class="n">chunk1contigs</span><span class="o">.</span><span class="n">alignmentset</span><span class="o">.</span><span class="n">xml</span>
<span class="n">variantCaller</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">alignmentSetRefWindows</span> <span class="o">--</span><span class="n">referenceFileName</span> <span class="n">referenceset</span><span class="o">.</span><span class="n">xml</span> <span class="o">--</span><span class="n">outputFilename</span> <span class="n">chunk2consensus</span><span class="o">.</span><span class="n">fasta</span> <span class="o">--</span><span class="n">algorithm</span> <span class="n">quiver</span> <span class="n">chunk2contigs</span><span class="o">.</span><span class="n">alignmentset</span><span class="o">.</span><span class="n">xml</span>

<span class="c"># Inside quiver (still using python dataset API):</span>
<span class="n">aln</span> <span class="o">=</span> <span class="n">AlignmentSet</span><span class="p">(</span><span class="n">fname</span><span class="p">)</span>
<span class="c"># get this set&#39;s windows:</span>
<span class="n">refWindows</span> <span class="o">=</span> <span class="n">aln</span><span class="o">.</span><span class="n">refWindows</span>
<span class="c"># gather the reads for these windows using readsInRange, e.g.:</span>
<span class="n">reads</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span><span class="n">aln</span><span class="o">.</span><span class="n">readsInRange</span><span class="p">(</span><span class="n">rId</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">)</span> <span class="k">for</span> <span class="n">rId</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span> <span class="ow">in</span> <span class="n">refWindows</span><span class="p">))</span>
</pre></div>
</div>
</li>
</ol>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">pbcore.io.dataset</a></li>
<li><a class="reference internal" href="#console-entry-point-usage">Console Entry Point Usage</a></li>
<li><a class="reference internal" href="#usage-examples">Usage Examples</a><ul>
<li><a class="reference internal" href="#filter-reads-cli-version">Filter Reads (CLI version)</a></li>
<li><a class="reference internal" href="#filter-reads-api-version">Filter Reads (API version)</a></li>
<li><a class="reference internal" href="#resequencing-pipeline-cli-version">Resequencing Pipeline (CLI version)</a><ul>
<li><a class="reference internal" href="#tangential-information">Tangential Information</a></li>
</ul>
</li>
<li><a class="reference internal" href="#resequencing-pipeline-api-version">Resequencing Pipeline (API version)</a></li>
</ul>
</li>
</ul>

  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/pbcoretools.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="nav-item nav-item-0"><a href="index.html">pbcoretools 0.1.0 documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &copy; Copyright 2011-2015, Pacific Biosciences.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.3.1.
    </div>
  </body>
</html>